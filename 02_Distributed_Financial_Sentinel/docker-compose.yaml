# ==========================================
# Airflow Docker Compose File
# ==========================================
# Startup sequence: postgres -> webserver + scheduler (parallel)
#
# Usage: (Make sure in project directory)
#   1. Create folder: mkdir -p data/postgres
#                     mkdir -p airflow/logs
#                     mkdir -p airflow/plugins
#   2. Set permissions: chmod 700 data/postgres
#   3. docker-compose up -d
#   4. Exam data folder: ls -la data/postgres
# ==========================================

version: '3.8'

services:
  # ----------------------------------------
  # 1. Core Database (Postgres)
  # Stores Airflow metadata and business data
  # ----------------------------------------
  postgres:
    image: postgres:13
    container_name: pipeline-db
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-airflow}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-airflow}
      - POSTGRES_DB=${POSTGRES_DB:-airflow}
    ports:
      # Exposed for remote workers (HK/JP) to write back data
      # Security Note: Protected by UFW/IPTables on host machine
      - "5432:5432"
    volumes:
      # Store PostgreSQL data in project directory for isolation
      - ./data/postgres:/var/lib/postgresql/data
      - ./infra/init-db:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

  # ----------------------------------------
  # 2. Airflow Webserver (UI)
  # ----------------------------------------
  airflow-webserver:
    image: apache/airflow:2.7.1
    container_name: airflow-webserver
    command: webserver

    # Airflow Web UI: http://<US_IP>:8080
    ports:
      - "8080:8080"

    environment:
      # Use LocalExecutor, suitable for small to medium workloads
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor

      # Database connection string (credentials from .env file)
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres/${POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False

      # Key for encrypting connection credentials
      # Reads AIRFLOW_FERNET_KEY from .env file
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}

      # Worker node IPs (passed from .env for DAGs and connection setup)
      - HK_IP=${HK_IP}
      - JP_IP=${JP_IP}
      - US_IP=${US_IP}
    volumes:
      # Mount project directories: DAGs, Logs, and Plugins
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins

      # Mounting Host SSH keys for remote execution capability
      # :ro means read-only (for security)
      # require SSH key pair configured on host machine
      - /root/.ssh:/home/airflow/.ssh:ro
    
    # Start only after postgres health check pass
    depends_on:
      postgres:
        condition: service_healthy
    restart: always

  # ----------------------------------------
  # 3. Airflow Scheduler
  # ----------------------------------------
  airflow-scheduler:
    image: apache/airflow:2.7.1
    container_name: airflow-scheduler
    command: scheduler

    # Same with webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres/${POSTGRES_DB:-airflow}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - HK_IP=${HK_IP}
      - JP_IP=${JP_IP}
      - US_IP=${US_IP}
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /root/.ssh:/home/airflow/.ssh:ro
    depends_on:
      postgres:
        condition: service_healthy
    restart: always

# Note: All data is stored in project directory for complete isolation
# No named volumes needed - using bind mounts for all persistent data