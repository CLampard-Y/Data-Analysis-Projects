{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed9a121",
   "metadata": {},
   "source": [
    "# 04 Uplift Modeling (S/T/X-Learner)\n",
    "\n",
    "This notebook follows Phase 2 MVP 2.4: CATE estimation via S-Learner / T-Learner / X-Learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89367063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UTC+8] Run timestamp: 2026-02-23 21:56:39 +0800\n",
      "Figures will be saved to: outputs\\figures\n",
      "Loaded: data\\processed\\hillstrom_features.csv\n",
      "Loaded PS vector from: data\\processed\\hillstrom_ps.csv\n",
      "df.shape=(64000, 16) | X.shape=(64000, 9) | train=(44800, 9) | test=(19200, 9) | T.mean(full)=0.6671 | T.mean(train)=0.6671 | T.mean(test)=0.6671\n",
      "ps.mean(full)=0.6671 | ps.std(full)=0.0039\n",
      "Covariates (n=9): ['recency', 'history', 'mens', 'womens', 'newbie', 'channel_Phone', 'channel_Web', 'zip_Surburban', 'zip_Urban']\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Section 0 (Cell 1/1): Setup\n",
    "# ======================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Timezone anchoring for reproducibility\n",
    "TZ_UTC8 = timezone(timedelta(hours=8))\n",
    "run_ts_utc8 = datetime.now(TZ_UTC8).strftime('%Y-%m-%d %H:%M:%S %z')\n",
    "print(f\"[UTC+8] Run timestamp: {run_ts_utc8}\")\n",
    "\n",
    "# Project root resolution (avoid hardcoding)\n",
    "project_root = Path.cwd()\n",
    "if project_root.name == 'notebooks':\n",
    "    project_root = project_root.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Plot style\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 13\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Load config (config.yml / config.yaml)\n",
    "config_candidates = [project_root / 'configs' / 'config.yaml', project_root / 'configs' / 'config.yml']\n",
    "config_path = next((p for p in config_candidates if p.exists()), None)\n",
    "if config_path is None:\n",
    "    raise FileNotFoundError(f\"No config file found. Searched: {[str(p) for p in config_candidates]}\")\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "features_path = Path(config['paths']['features_data'])\n",
    "assert features_path.exists(), f\"Feature file not found: {features_path}\"\n",
    "\n",
    "figures_dir = Path(config['paths'].get('figures_dir', 'outputs/figures/'))\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Figures will be saved to: {figures_dir}\")\n",
    "\n",
    "# Load features\n",
    "df = pd.read_csv(features_path)\n",
    "assert isinstance(df, pd.DataFrame) and len(df) > 0, \"Loaded features DataFrame is empty\"\n",
    "print(f\"Loaded: {features_path}\")\n",
    "\n",
    "treatment_col = config['data']['treatment_col']\n",
    "outcome_col = config['data']['outcome_col']\n",
    "spend_col = config['data']['spend_col']\n",
    "covariates = config['data']['covariates']\n",
    "\n",
    "missing_covs = [c for c in covariates if c not in df.columns]\n",
    "assert len(missing_covs) == 0, f\"Missing covariates in df: {missing_covs}\"\n",
    "assert treatment_col in df.columns, f\"Missing required column: {treatment_col}\"\n",
    "assert outcome_col in df.columns, f\"Missing required column: {outcome_col}\"\n",
    "assert spend_col in df.columns, f\"Missing required column: {spend_col}\"\n",
    "\n",
    "# Split X, T, Y\n",
    "X = df[covariates].copy()\n",
    "T = pd.to_numeric(df[treatment_col], errors='coerce').astype(int)\n",
    "Y = pd.to_numeric(df[outcome_col], errors='coerce').astype(int)\n",
    "\n",
    "assert X.isnull().sum().sum() == 0, \"X contains NaN\"\n",
    "assert set(pd.unique(T)).issubset({0, 1}), \"T must be binary (0/1)\"\n",
    "assert set(pd.unique(Y)).issubset({0, 1}), \"Y must be binary (0/1)\"\n",
    "\n",
    "# Load or compute PS vector\n",
    "# Requests \"load features + PS vector\"; if the persisted PS artifact is not found,\n",
    "# we compute PS via LogisticRegression (estimate_ps) and persist it for reproducibility.\n",
    "ps_path = Path(config['paths'].get('ps_data', 'data/processed/hillstrom_ps.csv'))\n",
    "ps = None\n",
    "if ps_path.exists():\n",
    "    ps_df = pd.read_csv(ps_path)\n",
    "    if 'ps' not in ps_df.columns:\n",
    "        raise ValueError(f\"PS file missing 'ps' column: {ps_path}\")\n",
    "    ps = pd.to_numeric(ps_df['ps'], errors='coerce').to_numpy(dtype=float)\n",
    "    if len(ps) != len(df):\n",
    "        raise ValueError(f\"PS length mismatch: len(ps)={len(ps)} vs len(df)={len(df)}\")\n",
    "    print(f\"Loaded PS vector from: {ps_path}\")\n",
    "else:\n",
    "    from src.causal import estimate_ps\n",
    "    ps, _ = estimate_ps(X, T, random_state=int(config.get('general', {}).get('random_state', 42)))\n",
    "    ps_out = pd.DataFrame({'ps': ps.astype(float)})\n",
    "    ps_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    ps_out.to_csv(ps_path, index=False)\n",
    "    print(f\"Computed PS vector and saved to: {ps_path}\")\n",
    "\n",
    "ps = np.asarray(ps, dtype=float).reshape(-1)\n",
    "assert np.isfinite(ps).all(), \"ps contains NaN/inf\"\n",
    "assert ps.min() >= 0.0 and ps.max() <= 1.0, \"ps must be within [0, 1]\"\n",
    "\n",
    "# Train/Test split (architecture review adjustment #6)\n",
    "X_train, X_test, T_train, T_test, Y_train, Y_test, ps_train, ps_test = train_test_split(\n",
    "    X,\n",
    "    T,\n",
    "    Y,\n",
    "    ps,\n",
    "    test_size=0.3,\n",
    "    random_state=int(config.get('general', {}).get('random_state', 42)),\n",
    "    stratify=T,\n",
    ")\n",
    "\n",
    "assert len(X_train) == len(T_train) == len(Y_train) == len(ps_train), \"Train split length mismatch\"\n",
    "assert len(X_test) == len(T_test) == len(Y_test) == len(ps_test), \"Test split length mismatch\"\n",
    "\n",
    "print(\n",
    "    f\"df.shape={df.shape} | X.shape={X.shape} | \"\n",
    "    f\"train={X_train.shape} | test={X_test.shape} | \"\n",
    "    f\"T.mean(full)={T.mean():.4f} | T.mean(train)={np.mean(T_train):.4f} | T.mean(test)={np.mean(T_test):.4f}\"\n",
    ")\n",
    "print(f\"ps.mean(full)={ps.mean():.4f} | ps.std(full)={ps.std():.4f}\")\n",
    "print(f\"Covariates (n={len(covariates)}): {covariates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449c757",
   "metadata": {},
   "source": [
    "## Section 1: S-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e4040e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>5%</th>\n",
       "      <th>50%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cate_s</th>\n",
       "      <td>19200.0</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>-0.011254</td>\n",
       "      <td>-0.003266</td>\n",
       "      <td>-0.000601</td>\n",
       "      <td>0.00377</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>0.018345</td>\n",
       "      <td>0.036114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      mean       std       min        1%        5%      50%  \\\n",
       "cate_s  19200.0  0.004402  0.004021 -0.011254 -0.003266 -0.000601  0.00377   \n",
       "\n",
       "             95%       99%       max  \n",
       "cate_s  0.011537  0.018345  0.036114  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(|cate_s|) = 0.036114\n",
      "Expected (RCT) rough range: [-0.02, 0.02] | observed min=-0.011254, max=0.036114\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Section 1 (Cell 1/1): Fit S-Learner on train, predict CATE on test\n",
    "# ======================================================\n",
    "\n",
    "import importlib\n",
    "import src.uplift\n",
    "importlib.reload(src.uplift)\n",
    "from src.uplift import fit_s_learner\n",
    "\n",
    "uplift_cfg = config.get('uplift', {})\n",
    "n_estimators = int(uplift_cfg.get('n_estimators', 100))\n",
    "max_depth = int(uplift_cfg.get('max_depth', 5))\n",
    "random_state = int(config.get('general', {}).get('random_state', 42))\n",
    "\n",
    "cate_s = fit_s_learner(\n",
    "    X_train,\n",
    "    T_train,\n",
    "    Y_train,\n",
    "    X_pred=X_test,\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "assert isinstance(cate_s, np.ndarray), 'cate_s must be a numpy.ndarray'\n",
    "assert len(cate_s) == len(X_test), 'CATE length must match test sample size'\n",
    "assert np.isfinite(cate_s).all(), 'CATE contains NaN/inf'\n",
    "\n",
    "cate_s_series = pd.Series(cate_s, name='cate_s')\n",
    "display(cate_s_series.describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]).to_frame().T)\n",
    "\n",
    "max_abs = float(np.max(np.abs(cate_s)))\n",
    "print(f\"max(|cate_s|) = {max_abs:.6f}\")\n",
    "if max_abs > 0.10:\n",
    "    print('[DQ] CATE magnitude > 0.10. Check model calibration / leakage / feature pipeline.')\n",
    "\n",
    "# Expected scale (Hillstrom RCT): roughly within [-0.02, 0.02]\n",
    "print(f\"Expected (RCT) rough range: [-0.02, 0.02] | observed min={cate_s.min():.6f}, max={cate_s.max():.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
